{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"Final_Combined_Notebook_GCP.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"FCL4-G1DY9JM","colab_type":"text"},"source":["Warning: If you attempt to run this file on a local machine, it will most likely take more than a day before it finishes.  Run it on Cloud is much better."]},{"cell_type":"markdown","metadata":{"id":"ftwiq7NxY9JO","colab_type":"text"},"source":["# Importing packages"]},{"cell_type":"code","metadata":{"id":"fLsGeH09Y9JP","colab_type":"code","colab":{}},"source":["# Importing necessary packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer, precision_score, recall_score,f1_score\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n","import statsmodels.api as sm\n","\n","# Dimensionality Reduction Packages\n","from sklearn.decomposition import PCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import KernelPCA\n","\n","# Models packages\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# writing to CSV  \n","import csv "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qu9qAKfwY9JW","colab_type":"text"},"source":["# 1. Data Exploration"]},{"cell_type":"code","metadata":{"id":"BgxeRmbVY9JX","colab_type":"code","outputId":"36ac7fc4-6eaf-4d04-957d-5c0727a09507","colab":{}},"source":["# Loading Dataset\n","df = pd.read_csv('UCI_Credit_Card.csv')\n","\n","pd.set_option('display.max_columns', 999)\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>LIMIT_BAL</th>\n","      <th>SEX</th>\n","      <th>EDUCATION</th>\n","      <th>MARRIAGE</th>\n","      <th>AGE</th>\n","      <th>PAY_0</th>\n","      <th>PAY_2</th>\n","      <th>PAY_3</th>\n","      <th>PAY_4</th>\n","      <th>PAY_5</th>\n","      <th>PAY_6</th>\n","      <th>BILL_AMT1</th>\n","      <th>BILL_AMT2</th>\n","      <th>BILL_AMT3</th>\n","      <th>BILL_AMT4</th>\n","      <th>BILL_AMT5</th>\n","      <th>BILL_AMT6</th>\n","      <th>PAY_AMT1</th>\n","      <th>PAY_AMT2</th>\n","      <th>PAY_AMT3</th>\n","      <th>PAY_AMT4</th>\n","      <th>PAY_AMT5</th>\n","      <th>PAY_AMT6</th>\n","      <th>default.payment.next.month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>20000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-2</td>\n","      <td>-2</td>\n","      <td>3913.0</td>\n","      <td>3102.0</td>\n","      <td>689.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>689.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>120000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2682.0</td>\n","      <td>1725.0</td>\n","      <td>2682.0</td>\n","      <td>3272.0</td>\n","      <td>3455.0</td>\n","      <td>3261.0</td>\n","      <td>0.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>0.0</td>\n","      <td>2000.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>90000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29239.0</td>\n","      <td>14027.0</td>\n","      <td>13559.0</td>\n","      <td>14331.0</td>\n","      <td>14948.0</td>\n","      <td>15549.0</td>\n","      <td>1518.0</td>\n","      <td>1500.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>5000.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>50000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>46990.0</td>\n","      <td>48233.0</td>\n","      <td>49291.0</td>\n","      <td>28314.0</td>\n","      <td>28959.0</td>\n","      <td>29547.0</td>\n","      <td>2000.0</td>\n","      <td>2019.0</td>\n","      <td>1200.0</td>\n","      <td>1100.0</td>\n","      <td>1069.0</td>\n","      <td>1000.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>50000.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>57</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8617.0</td>\n","      <td>5670.0</td>\n","      <td>35835.0</td>\n","      <td>20940.0</td>\n","      <td>19146.0</td>\n","      <td>19131.0</td>\n","      <td>2000.0</td>\n","      <td>36681.0</td>\n","      <td>10000.0</td>\n","      <td>9000.0</td>\n","      <td>689.0</td>\n","      <td>679.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n","0   1    20000.0    2          2         1   24      2      2     -1     -1   \n","1   2   120000.0    2          2         2   26     -1      2      0      0   \n","2   3    90000.0    2          2         2   34      0      0      0      0   \n","3   4    50000.0    2          2         1   37      0      0      0      0   \n","4   5    50000.0    1          2         1   57     -1      0     -1      0   \n","\n","   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n","0     -2     -2     3913.0     3102.0      689.0        0.0        0.0   \n","1      0      2     2682.0     1725.0     2682.0     3272.0     3455.0   \n","2      0      0    29239.0    14027.0    13559.0    14331.0    14948.0   \n","3      0      0    46990.0    48233.0    49291.0    28314.0    28959.0   \n","4      0      0     8617.0     5670.0    35835.0    20940.0    19146.0   \n","\n","   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n","0        0.0       0.0     689.0       0.0       0.0       0.0       0.0   \n","1     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0   \n","2    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0   \n","3    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0   \n","4    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0   \n","\n","   default.payment.next.month  \n","0                           1  \n","1                           1  \n","2                           0  \n","3                           0  \n","4                           0  "]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"UcI1tqYJY9Jg","colab_type":"text"},"source":["##### There are total of 25 columns with numeric values. "]},{"cell_type":"markdown","metadata":{"id":"5-j0DWGeY9Jh","colab_type":"text"},"source":["### 1.1 Features Reference\n","\n","__ID:__ ID of each customer\n","<br>\n","__LIMIT_BAL:__ Amount of the given credit (NT dollar). It includes both the individual consumer credit and his/her family (supplementary) credit.\n","<br>\n","__SEX:__ Gender (__1 =__ male, __2 =__ female)\n","<br>\n","__EDUCATION:__ (__1 =__ graduate school, __2 =__ university, __3 =__ high school, __4 =__ others, __5 =__ unknown, __6 =__ unknown, __0 =__ unknown)\n","<br>\n","__MARRIAGE:__ (__1 =__ married, __2 =__ single, __3 =__ others, __0 =__ unknown)\n","<br>\n","__AGE:__ Age (year)\n","<br>\n","__PAY_0:__ Repayment status in September 2005 (__-2 =__ no consumption, __-1 =__ paid in full, __0 =__ use of revolving credit, __1 =__ payment delay for one month, __2 =__ payment delay for two months, __3 =__ payment delay for three months ... __8 =__ payment delay for eight months, __9 =__ payment delay for nine months and above)\n","<br>\n","__PAY_2:__ Repayment status in August 2005 (__scale same as above__)\n","<br>\n","__PAY_3:__ Repayment status in July 2005 (__scale same as above__)\n","<br>\n","__PAY_4:__ Repayment status in June 2005 (__scale same as above__)\n","<br>\n","__PAY_5:__ Repayment status in May 2005 (__scale same as above__)\n","<br>\n","__PAY_6:__ Repayment status in April 2005 (__scale same as above__)\n","<br>\n","__BILL_AMT1:__ Amount of bill statement in September, 2005 (NT dollar)\n","<br>\n","__BILL_AMT2:__ Amount of bill statement in August, 2005 (NT dollar)\n","<br>\n","__BILL_AMT3:__ Amount of bill statement in July, 2005 (NT dollar)\n","<br>\n","__BILL_AMT4:__ Amount of bill statement in June, 2005 (NT dollar)\n","<br>\n","__BILL_AMT5:__ Amount of bill statement in May, 2005 (NT dollar)\n","<br>\n","__BILL_AMT6:__ Amount of bill statement in April, 2005 (NT dollar)\n","<br>\n","__PAY_AMT1:__ Amount of previous payment in September, 2005 (NT dollar)\n","<br>\n","__PAY_AMT2:__ Amount of previous payment in August, 2005 (NT dollar)\n","<br>\n","__PAY_AMT3:__ Amount of previous payment in July, 2005 (NT dollar)\n","<br>\n","__PAY_AMT4:__ Amount of previous payment in June, 2005 (NT dollar)\n","<br>\n","__PAY_AMT5:__ Amount of previous payment in May, 2005 (NT dollar)\n","<br>\n","__PAY_AMT6:__ Amount of previous payment in April, 2005 (NT dollar)\n","<br>\n","__default.payment.next.month:__ Default payment (__1 =__ yes, __0 =__ no)"]},{"cell_type":"markdown","metadata":{"id":"y5nARi3zY9Ji","colab_type":"text"},"source":["# 1.2 Dataset Information"]},{"cell_type":"code","metadata":{"id":"gNkRed0SY9Jj","colab_type":"code","outputId":"1afba4a8-43e2-4f5b-ee7e-eb28b39e7579","colab":{}},"source":["df.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 30000 entries, 0 to 29999\n","Data columns (total 25 columns):\n","ID                            30000 non-null int64\n","LIMIT_BAL                     30000 non-null float64\n","SEX                           30000 non-null int64\n","EDUCATION                     30000 non-null int64\n","MARRIAGE                      30000 non-null int64\n","AGE                           30000 non-null int64\n","PAY_0                         30000 non-null int64\n","PAY_2                         30000 non-null int64\n","PAY_3                         30000 non-null int64\n","PAY_4                         30000 non-null int64\n","PAY_5                         30000 non-null int64\n","PAY_6                         30000 non-null int64\n","BILL_AMT1                     30000 non-null float64\n","BILL_AMT2                     30000 non-null float64\n","BILL_AMT3                     30000 non-null float64\n","BILL_AMT4                     30000 non-null float64\n","BILL_AMT5                     30000 non-null float64\n","BILL_AMT6                     30000 non-null float64\n","PAY_AMT1                      30000 non-null float64\n","PAY_AMT2                      30000 non-null float64\n","PAY_AMT3                      30000 non-null float64\n","PAY_AMT4                      30000 non-null float64\n","PAY_AMT5                      30000 non-null float64\n","PAY_AMT6                      30000 non-null float64\n","default.payment.next.month    30000 non-null int64\n","dtypes: float64(13), int64(12)\n","memory usage: 5.7 MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"izmi5UhYY9Jp","colab_type":"text"},"source":["##### There are total of 30,000 rows and 25 columns. Columns have integer and float data types. There are no missing values in the dataset."]},{"cell_type":"code","metadata":{"id":"HwdXBzBJY9Jq","colab_type":"code","outputId":"7aa9a6c3-4f6d-4a4d-a5ca-154c2d33123b","colab":{}},"source":["df.describe().T"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ID</th>\n","      <td>30000.0</td>\n","      <td>15000.500000</td>\n","      <td>8660.398374</td>\n","      <td>1.0</td>\n","      <td>7500.75</td>\n","      <td>15000.5</td>\n","      <td>22500.25</td>\n","      <td>30000.0</td>\n","    </tr>\n","    <tr>\n","      <th>LIMIT_BAL</th>\n","      <td>30000.0</td>\n","      <td>167484.322667</td>\n","      <td>129747.661567</td>\n","      <td>10000.0</td>\n","      <td>50000.00</td>\n","      <td>140000.0</td>\n","      <td>240000.00</td>\n","      <td>1000000.0</td>\n","    </tr>\n","    <tr>\n","      <th>SEX</th>\n","      <td>30000.0</td>\n","      <td>1.603733</td>\n","      <td>0.489129</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>2.0</td>\n","      <td>2.00</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>EDUCATION</th>\n","      <td>30000.0</td>\n","      <td>1.853133</td>\n","      <td>0.790349</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>2.0</td>\n","      <td>2.00</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>MARRIAGE</th>\n","      <td>30000.0</td>\n","      <td>1.551867</td>\n","      <td>0.521970</td>\n","      <td>0.0</td>\n","      <td>1.00</td>\n","      <td>2.0</td>\n","      <td>2.00</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>AGE</th>\n","      <td>30000.0</td>\n","      <td>35.485500</td>\n","      <td>9.217904</td>\n","      <td>21.0</td>\n","      <td>28.00</td>\n","      <td>34.0</td>\n","      <td>41.00</td>\n","      <td>79.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_0</th>\n","      <td>30000.0</td>\n","      <td>-0.016700</td>\n","      <td>1.123802</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_2</th>\n","      <td>30000.0</td>\n","      <td>-0.133767</td>\n","      <td>1.197186</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_3</th>\n","      <td>30000.0</td>\n","      <td>-0.166200</td>\n","      <td>1.196868</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_4</th>\n","      <td>30000.0</td>\n","      <td>-0.220667</td>\n","      <td>1.169139</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_5</th>\n","      <td>30000.0</td>\n","      <td>-0.266200</td>\n","      <td>1.133187</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_6</th>\n","      <td>30000.0</td>\n","      <td>-0.291100</td>\n","      <td>1.149988</td>\n","      <td>-2.0</td>\n","      <td>-1.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT1</th>\n","      <td>30000.0</td>\n","      <td>51223.330900</td>\n","      <td>73635.860576</td>\n","      <td>-165580.0</td>\n","      <td>3558.75</td>\n","      <td>22381.5</td>\n","      <td>67091.00</td>\n","      <td>964511.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT2</th>\n","      <td>30000.0</td>\n","      <td>49179.075167</td>\n","      <td>71173.768783</td>\n","      <td>-69777.0</td>\n","      <td>2984.75</td>\n","      <td>21200.0</td>\n","      <td>64006.25</td>\n","      <td>983931.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT3</th>\n","      <td>30000.0</td>\n","      <td>47013.154800</td>\n","      <td>69349.387427</td>\n","      <td>-157264.0</td>\n","      <td>2666.25</td>\n","      <td>20088.5</td>\n","      <td>60164.75</td>\n","      <td>1664089.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT4</th>\n","      <td>30000.0</td>\n","      <td>43262.948967</td>\n","      <td>64332.856134</td>\n","      <td>-170000.0</td>\n","      <td>2326.75</td>\n","      <td>19052.0</td>\n","      <td>54506.00</td>\n","      <td>891586.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT5</th>\n","      <td>30000.0</td>\n","      <td>40311.400967</td>\n","      <td>60797.155770</td>\n","      <td>-81334.0</td>\n","      <td>1763.00</td>\n","      <td>18104.5</td>\n","      <td>50190.50</td>\n","      <td>927171.0</td>\n","    </tr>\n","    <tr>\n","      <th>BILL_AMT6</th>\n","      <td>30000.0</td>\n","      <td>38871.760400</td>\n","      <td>59554.107537</td>\n","      <td>-339603.0</td>\n","      <td>1256.00</td>\n","      <td>17071.0</td>\n","      <td>49198.25</td>\n","      <td>961664.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT1</th>\n","      <td>30000.0</td>\n","      <td>5663.580500</td>\n","      <td>16563.280354</td>\n","      <td>0.0</td>\n","      <td>1000.00</td>\n","      <td>2100.0</td>\n","      <td>5006.00</td>\n","      <td>873552.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT2</th>\n","      <td>30000.0</td>\n","      <td>5921.163500</td>\n","      <td>23040.870402</td>\n","      <td>0.0</td>\n","      <td>833.00</td>\n","      <td>2009.0</td>\n","      <td>5000.00</td>\n","      <td>1684259.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT3</th>\n","      <td>30000.0</td>\n","      <td>5225.681500</td>\n","      <td>17606.961470</td>\n","      <td>0.0</td>\n","      <td>390.00</td>\n","      <td>1800.0</td>\n","      <td>4505.00</td>\n","      <td>896040.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT4</th>\n","      <td>30000.0</td>\n","      <td>4826.076867</td>\n","      <td>15666.159744</td>\n","      <td>0.0</td>\n","      <td>296.00</td>\n","      <td>1500.0</td>\n","      <td>4013.25</td>\n","      <td>621000.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT5</th>\n","      <td>30000.0</td>\n","      <td>4799.387633</td>\n","      <td>15278.305679</td>\n","      <td>0.0</td>\n","      <td>252.50</td>\n","      <td>1500.0</td>\n","      <td>4031.50</td>\n","      <td>426529.0</td>\n","    </tr>\n","    <tr>\n","      <th>PAY_AMT6</th>\n","      <td>30000.0</td>\n","      <td>5215.502567</td>\n","      <td>17777.465775</td>\n","      <td>0.0</td>\n","      <td>117.75</td>\n","      <td>1500.0</td>\n","      <td>4000.00</td>\n","      <td>528666.0</td>\n","    </tr>\n","    <tr>\n","      <th>default.payment.next.month</th>\n","      <td>30000.0</td>\n","      <td>0.221200</td>\n","      <td>0.415062</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              count           mean            std       min  \\\n","ID                          30000.0   15000.500000    8660.398374       1.0   \n","LIMIT_BAL                   30000.0  167484.322667  129747.661567   10000.0   \n","SEX                         30000.0       1.603733       0.489129       1.0   \n","EDUCATION                   30000.0       1.853133       0.790349       0.0   \n","MARRIAGE                    30000.0       1.551867       0.521970       0.0   \n","AGE                         30000.0      35.485500       9.217904      21.0   \n","PAY_0                       30000.0      -0.016700       1.123802      -2.0   \n","PAY_2                       30000.0      -0.133767       1.197186      -2.0   \n","PAY_3                       30000.0      -0.166200       1.196868      -2.0   \n","PAY_4                       30000.0      -0.220667       1.169139      -2.0   \n","PAY_5                       30000.0      -0.266200       1.133187      -2.0   \n","PAY_6                       30000.0      -0.291100       1.149988      -2.0   \n","BILL_AMT1                   30000.0   51223.330900   73635.860576 -165580.0   \n","BILL_AMT2                   30000.0   49179.075167   71173.768783  -69777.0   \n","BILL_AMT3                   30000.0   47013.154800   69349.387427 -157264.0   \n","BILL_AMT4                   30000.0   43262.948967   64332.856134 -170000.0   \n","BILL_AMT5                   30000.0   40311.400967   60797.155770  -81334.0   \n","BILL_AMT6                   30000.0   38871.760400   59554.107537 -339603.0   \n","PAY_AMT1                    30000.0    5663.580500   16563.280354       0.0   \n","PAY_AMT2                    30000.0    5921.163500   23040.870402       0.0   \n","PAY_AMT3                    30000.0    5225.681500   17606.961470       0.0   \n","PAY_AMT4                    30000.0    4826.076867   15666.159744       0.0   \n","PAY_AMT5                    30000.0    4799.387633   15278.305679       0.0   \n","PAY_AMT6                    30000.0    5215.502567   17777.465775       0.0   \n","default.payment.next.month  30000.0       0.221200       0.415062       0.0   \n","\n","                                 25%       50%        75%        max  \n","ID                           7500.75   15000.5   22500.25    30000.0  \n","LIMIT_BAL                   50000.00  140000.0  240000.00  1000000.0  \n","SEX                             1.00       2.0       2.00        2.0  \n","EDUCATION                       1.00       2.0       2.00        6.0  \n","MARRIAGE                        1.00       2.0       2.00        3.0  \n","AGE                            28.00      34.0      41.00       79.0  \n","PAY_0                          -1.00       0.0       0.00        8.0  \n","PAY_2                          -1.00       0.0       0.00        8.0  \n","PAY_3                          -1.00       0.0       0.00        8.0  \n","PAY_4                          -1.00       0.0       0.00        8.0  \n","PAY_5                          -1.00       0.0       0.00        8.0  \n","PAY_6                          -1.00       0.0       0.00        8.0  \n","BILL_AMT1                    3558.75   22381.5   67091.00   964511.0  \n","BILL_AMT2                    2984.75   21200.0   64006.25   983931.0  \n","BILL_AMT3                    2666.25   20088.5   60164.75  1664089.0  \n","BILL_AMT4                    2326.75   19052.0   54506.00   891586.0  \n","BILL_AMT5                    1763.00   18104.5   50190.50   927171.0  \n","BILL_AMT6                    1256.00   17071.0   49198.25   961664.0  \n","PAY_AMT1                     1000.00    2100.0    5006.00   873552.0  \n","PAY_AMT2                      833.00    2009.0    5000.00  1684259.0  \n","PAY_AMT3                      390.00    1800.0    4505.00   896040.0  \n","PAY_AMT4                      296.00    1500.0    4013.25   621000.0  \n","PAY_AMT5                      252.50    1500.0    4031.50   426529.0  \n","PAY_AMT6                      117.75    1500.0    4000.00   528666.0  \n","default.payment.next.month      0.00       0.0       0.00        1.0  "]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"8ueueUOtY9Jv","colab_type":"text"},"source":["### 1.4 Visualizations"]},{"cell_type":"code","metadata":{"id":"wrKNEEUtY9Jw","colab_type":"code","colab":{}},"source":["# Plotting distribution of class label\n","plt.hist(df['default.payment.next.month'], bins=[-.5,.5,1.5], ec=\"k\")\n","plt.xticks((0,1))\n","plt.title('Distribution of Class Label. \\n Not Default = 0 || Default = 1')\n","plt.ylabel('Count')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nr0Qi12ZY9J4","colab_type":"text"},"source":["##### As we see, this is an imbalanced dataset with only 22% Default cases and 78% Non-default cases. It means that if we make a model that always predicts 0 (non-default), we still get 78% accuracy rate. Of course, this kind of model would be useless for the banks to predict the defaulters next month. "]},{"cell_type":"code","metadata":{"id":"IV--aBCtY9J5","colab_type":"code","colab":{}},"source":["df.SEX.value_counts().plot(kind='bar')\n","plt.title('SEX \\n 1 = male || 2 = female')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEzi2Th0Y9KB","colab_type":"text"},"source":["##### There are more felmales than males in the dataset. We can change the values to binary values later (One Hot Encoding and drop one). We will address this in Data Preparation section of this notebook.\n"]},{"cell_type":"code","metadata":{"id":"0KWnQL4IY9KC","colab_type":"code","colab":{}},"source":["df.MARRIAGE.value_counts().plot(kind='bar')\n","plt.title('MARRIAGE \\n 0 = unknown || 1 = married || 2 = single || 3 = others')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_17WI6yGY9KH","colab_type":"text"},"source":["##### There are only few cases of labels '0' and '3'. We can combine the rare cases as a single value later. We will address this in Data Preparation section of this notebook."]},{"cell_type":"code","metadata":{"id":"esxuE5HzY9KI","colab_type":"code","colab":{}},"source":["df.EDUCATION.value_counts().plot(kind='bar')\n","plt.title('EDUCATION \\n 0 = unknown || 1 = graduate school || 2 = univeristy || 3 = high school || 4 = others || 5 and 6 = unknown')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TA0NqFxgY9KN","colab_type":"text"},"source":["##### There are few cases of undocumented values in \"EDUCATION\" column. We can add up all these instances and label them as a single value \"others\". For instance we can merge labels \"4\", \"5\", \"6\" and \"0\". We will address this in Data Preparation section of this notebook."]},{"cell_type":"code","metadata":{"id":"XMaP_tXVY9KO","colab_type":"code","colab":{}},"source":["# Plotting distribution of class label\n","plt.hist(df['LIMIT_BAL'], bins=60)\n","plt.title('Distribution of Credit Limits')\n","plt.ylabel('Count')\n","plt.xlabel('Credit Limit')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuSrZCjDY9KW","colab_type":"text"},"source":["##### This is skewed to the right. Most of the customers are issued loan amount of 50,000 NT Dollars (around 2,200 US Dollars today with inflation). And it appears to be there are some outliers such as amount of 1,000,000 NT issued to only 1 customer. "]},{"cell_type":"code","metadata":{"id":"wPu07fg_Y9KX","colab_type":"code","colab":{}},"source":["# SEX: Converting value \"2\" to \"0\" so that we have only binary values. \n","#      After conversion 0 represents \"female\", and 1 represents \"male\".\n","\n","df[\"SEX\"].replace({2: 0}, inplace=True)\n","\n","# MARRIAGE: Since there are only few values of \"3\" and \"0\", we are converting value \"3\" to \"0\". \n","#           After conversion, \"0\" will represent \"others\".\n","\n","df[\"MARRIAGE\"].replace({3: 0}, inplace=True)\n","\n","# EDUCATION: Converting values \"4\", \"5\", and \"6\" to \"0\". After conversion \"0\" will represent \"others\"\n","df[\"EDUCATION\"].replace({4: 0}, inplace=True)\n","df[\"EDUCATION\"].replace({5: 0}, inplace=True)\n","df[\"EDUCATION\"].replace({6: 0}, inplace=True)\n","\n","# OneHot Encoding Columns MARRIAGE and EDUCATION\n","df=pd.get_dummies(df, prefix=['EDUCATION'], columns=['EDUCATION'])\n","df=pd.get_dummies(df, prefix=['MARRIAGE'], columns=['MARRIAGE'])\n","\n","# Changing class label column name to 'Class'. Original name is too long to type later.\n","df=df.rename(columns = {\"default.payment.next.month\": \"Class\"})\n","\n","# We are dropping the rows where BILL_AMT and PAY_AMT columns have all 0 values and yet class label is 1. This does \n","# not make much sense. How can a customer default while his/her Bill amount for a certain month is 0?\n","for i in df.ID:\n","    if df.BILL_AMT1[i-1]==0 and \\\n","    df.BILL_AMT2[i-1]==0 and \\\n","    df.BILL_AMT3[i-1]==0 and \\\n","    df.BILL_AMT4[i-1]==0 and \\\n","    df.BILL_AMT5[i-1]==0 and \\\n","    df.BILL_AMT6[i-1]==0 and \\\n","    df.PAY_AMT1[i-1]==0 and \\\n","    df.PAY_AMT2[i-1]==0 and \\\n","    df.PAY_AMT3[i-1]==0 and \\\n","    df.PAY_AMT4[i-1]==0 and \\\n","    df.PAY_AMT5[i-1]==0 and \\\n","    df.PAY_AMT6[i-1]==0 and \\\n","    df.Class[i-1]==1:\n","        df.drop(i-1, inplace=True)\n","\n","# Finally, we are reorganizing the columns and drop some columns which we do not need in our dataset. For instance, we do not\n","# need ID column and we can remove one of the one-hot encoded column from each categorical feature.\n","\n","df = df[['LIMIT_BAL', 'SEX', 'EDUCATION_1', 'EDUCATION_2', 'EDUCATION_3', 'MARRIAGE_1', 'MARRIAGE_2', 'AGE',\\\n","         'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', \\\n","        'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', \\\n","        'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'Class']]\n","\n","# Excluded columns from Dataset: ID, EDUCATION_0, MARRIAGE_0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z60Y9OdlY9Ke","colab_type":"code","outputId":"301cd833-19b8-4068-c078-5a82a4b86671","colab":{}},"source":["# Let's see how it looks like\n","print(df.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   LIMIT_BAL  SEX  EDUCATION_1  EDUCATION_2  EDUCATION_3  MARRIAGE_1  \\\n","0    20000.0    0            0            1            0           1   \n","1   120000.0    0            0            1            0           0   \n","2    90000.0    0            0            1            0           0   \n","3    50000.0    0            0            1            0           1   \n","4    50000.0    1            0            1            0           1   \n","\n","   MARRIAGE_2  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  BILL_AMT1  \\\n","0           0   24      2      2     -1     -1     -2     -2     3913.0   \n","1           1   26     -1      2      0      0      0      2     2682.0   \n","2           1   34      0      0      0      0      0      0    29239.0   \n","3           0   37      0      0      0      0      0      0    46990.0   \n","4           0   57     -1      0     -1      0      0      0     8617.0   \n","\n","   BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n","0     3102.0      689.0        0.0        0.0        0.0       0.0     689.0   \n","1     1725.0     2682.0     3272.0     3455.0     3261.0       0.0    1000.0   \n","2    14027.0    13559.0    14331.0    14948.0    15549.0    1518.0    1500.0   \n","3    48233.0    49291.0    28314.0    28959.0    29547.0    2000.0    2019.0   \n","4     5670.0    35835.0    20940.0    19146.0    19131.0    2000.0   36681.0   \n","\n","   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  Class  \n","0       0.0       0.0       0.0       0.0      1  \n","1    1000.0    1000.0       0.0    2000.0      1  \n","2    1000.0    1000.0    1000.0    5000.0      0  \n","3    1200.0    1100.0    1069.0    1000.0      0  \n","4   10000.0    9000.0     689.0     679.0      0  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HFBtdg8_Y9Kk","colab_type":"code","colab":{}},"source":["# First, let's define X and y. \n","X = df.iloc[:,0:-1].values\n","y = df.iloc[:, -1].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CjbcFX6Y9Kp","colab_type":"code","colab":{}},"source":["sc_X = StandardScaler()\n","X = sc_X.fit_transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ry_YcGy5Y9Kw","colab_type":"code","colab":{}},"source":["import random\n","\n","# Create random_state  \n","random_state = random.randint(0,101)\n","\n","# Splitting the dataframe into Train (70%) and Test (30%) Sets\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = random_state)\n","\n","# X_train = sc_X.fit_transform(X_train)\n","# X_test = sc_X.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7mSQJzdY9K1","colab_type":"code","colab":{}},"source":["#Regularizing how many ways to split the data during Randomized Search\n","cvNumber=5;\n","#Defining Accuracy Score as the criteria\n","ScoreCriteria='Accuracy Score'\n","file_name='ScoreTracker.csv'\n","Column_Header=['Data Extraction/Selection Method', 'Model Name', ScoreCriteria, 'Hyperparameters Setting']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m045bnosY9K6","colab_type":"code","colab":{}},"source":["with open(file_name, 'w', newline='') as csvfile: \n","    csvwriter = csv.writer(csvfile)\n","    csvwriter.writerow(Column_Header)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soom1PVxY9LA","colab_type":"text"},"source":["# Supporting Function Section"]},{"cell_type":"code","metadata":{"id":"vKz1vvY4Y9LC","colab_type":"code","colab":{}},"source":["# Backward Elimination Function\n","def backwardElimination(x, sl):\n","    numVars = len(x[0])\n","    for i in range(0, numVars):\n","        obj_OLS = sm.OLS(y, x).fit()\n","        maxVar = max(obj_OLS.pvalues).astype(float)\n","        if maxVar > sl:\n","            for j in range(0, numVars - i):\n","                if (obj_OLS.pvalues[j].astype(float) == maxVar):\n","                    x = np.delete(x, j, 1)\n","    print(obj_OLS.summary())\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKGeA66hY9LJ","colab_type":"code","colab":{}},"source":["#PCA\n","def myPCA(X_Values):\n","    #Creating the pca based on the criteria that it retains 99% of the variance within the data.\n","    pcaObj = PCA(n_components=0.99, whiten=True)\n","    X_Transformed = pcaObj.fit_transform(X_Values)\n","    return X_Transformed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGpsnqcYY9LO","colab_type":"code","colab":{}},"source":["#LDA\n","def myLDA(X_Values):\n","    ldaObj = LDA(n_components=None)\n","    X_Transformed = ldaObj.fit_transform(X_Values,y)\n","    return X_Transformed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqZ3p8kzY9LS","colab_type":"code","colab":{}},"source":["#Kernel PCA.  DO NOT RUN THIS, unless you have 64GB+ Ram it will crash, and it will be super slow!!!\n","\n","def mykernelPCA(X_Values):\n","    kernelPCAObj=KernelPCA(n_components=2, kernel='rbf')\n","    X_Transformed = kernelPCAObj.fit_transform(X_Values);\n","    return X_Transformed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R8XIc-hY9LX","colab_type":"code","colab":{}},"source":["#Function for writing files.\n","def writingFiles(DataProcess, ModelName,Hyperparameters, Scores):\n","    rows = [DataProcess, ModelName, Hyperparameters ,str(Scores['ac_score']),str(Scores['rec_score']),str(Scores['pre_score']),str(Scores['F1_score'])]\n","    FinalModelWriter.writerow(rows)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHySFhnTY9Le","colab_type":"code","colab":{}},"source":["#Function for doing kFold.  \n","def kFoldAccuracy(estimator_obj, X_data, y_data):\n","    #Making the kFold\n","    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n","    \n","    #Define evaluating parameters\n","    Scores = {\n","        'ac_score': make_scorer(accuracy_score), #These are callback functions\n","        'pre_score': make_scorer(precision_score),\n","        'rec_score': make_scorer(recall_score),\n","        'F1_score': make_scorer(f1_score) \n","    } \n","    \n","    #Make prediction\n","    modelScore = cross_validate(estimator=estimator_obj, X=X_data, y=y_data, cv=kf, scoring=Scores)\n","    \n","    for i in Scores:\n","        testScore = 'test_'+i\n","        Scores[i] = np.mean(modelScore[testScore])\n","\n","    return Scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu8yEAIMY9Lj","colab_type":"code","colab":{}},"source":["#Function for doing RandomizedSearchCV\n","\n","def randomSearch(X_Data,estimatorObj, parameters):\n","    #Scores=['accuracy', 'precision', 'recall', 'f1'] //may put this in later...\n","    rd_sr = RandomizedSearchCV(estimator=estimatorObj, param_distributions=parameters, scoring='accuracy', cv=cvNumber, n_jobs=-1)\n","    rd_sr.fit(X_Data,y)\n","    return [rd_sr.best_params_, round(rd_sr.best_score_,4)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ravbRYyRY9Lp","colab_type":"code","colab":{}},"source":["#Output to CSV file and also save it to the dictionary here.\n","def writeFiles(DataProcessMethod, rowDict):\n","    dictKey=DataProcessMethod + '_' + str(rowDict['name']) + '_' + str(rowDict['Param_Settings'])\n","    resultDict[dictKey]=rowDict[ScoreCriteria]\n","    \n","    with open(file_name, 'a', newline='') as csvfile:\n","        filewriter=csv.writer(csvfile)\n","        rows = [DataProcessMethod, rowDict['name'], str(rowDict[ScoreCriteria]), rowDict['Param_Settings']]\n","        filewriter.writerow(rows)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9wlMBbhY9Lu","colab_type":"text"},"source":["Using Regression_And_WriteFiles to automate the process"]},{"cell_type":"code","metadata":{"id":"UV2CGBaIY9Lv","colab_type":"code","colab":{}},"source":["def Regression_And_WriteFiles(X_Data, DataProcessName, RegressionType):\n","    \n","    #1: Logistic Regression\n","    #2: kNN\n","    #3: SVM\n","    #4: Naive Bayes\n","    #5: Decision Tree\n","    #6: Random Forest\n","    if RegressionType==1: #Python doesn't seem to have switch statements.\n","        row=Logistic_Regression(X_Data)\n","        writeFiles(DataProcessName, row)\n","    elif RegressionType==2:\n","        row=kNN_Classifier(X_Data)\n","        writeFiles(DataProcessName, row)\n","    elif RegressionType==3:\n","        #Skipping this model because it simply takes way too long.  VM ran for 5 hours without getting results on this one.\n","        if DataProcessName=='LDA': \n","            return;\n","        else:\n","            row=SVM_Classifier(X_Data)\n","            writeFiles(DataProcessName, row)\n","    elif RegressionType==4:\n","        row=NaiveBaynes(X_Data)\n","        writeFiles(DataProcessName, row)\n","    elif RegressionType==5:\n","        row=DecisionTreeClassifier1(X_Data)\n","        writeFiles(DataProcessName, row)\n","    else:\n","        row=RandomForestClassifier1(X_Data)\n","        writeFiles(DataProcessName, row)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Cbu-EqFY9Lz","colab_type":"text"},"source":["# Various machine learning models and hyperparameters tuning"]},{"cell_type":"code","metadata":{"id":"4oKXvWflY9L0","colab_type":"code","colab":{}},"source":["def Logistic_Regression(X):\n","    regObj = LogisticRegression()\n","    \n","    random_param_lr = {\n","        'C':[0.001, 0.01, 0.09, 1, 5, 10, 25],\n","        'class_weight': [None, 'balanced', {0:1, 1:2}, {0:1, 1:3}, {0:2, 1:1}, {0:3, 1:1}, {0:2, 1:3}, {0:3, 1:2}]\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"Logistic Regression\"\n","    best_parameters, score = (randomSearch(X,regObj, random_param_lr))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxtHo2EBY9L6","colab_type":"code","colab":{}},"source":["def kNN_Classifier(X):\n","    kNNObj = KNeighborsClassifier()\n","    \n","    random_param = {\n","        'n_neighbors': [2, 5, 10, 15, 20, 25],\n","        'metric': ['minkowski', 'manhattan', 'euclidean'] #this probably isn't needed\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"kNN\"\n","    best_parameters, score = (randomSearch(X,kNNObj, random_param))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUnUFKnWY9MC","colab_type":"code","colab":{}},"source":["def SVM_Classifier(X):\n","    SVMobj = SVC()\n","    \n","    random_param = {\n","        'kernel': ['linear', 'rbf', 'poly'],\n","        'degree': [2, 3, 4, 5, 8]\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"SVM\"\n","    best_parameters, score = (randomSearch(X,SVMobj, random_param))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDthYgvSY9MH","colab_type":"code","colab":{}},"source":["def NaiveBaynes(X):\n","    NaiveBaynes = GaussianNB()\n","    \n","    random_param = {\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"Naive Baynes\"\n","    best_parameters, score = (randomSearch(X,NaiveBaynes, random_param))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbeUVdm3Y9MM","colab_type":"code","colab":{}},"source":["def DecisionTreeClassifier1(X):\n","    decTreeObj = DecisionTreeClassifier()\n","    \n","    random_param = {\n","        'criterion': ['gini', 'entropy'],\n","        'min_samples_leaf' : [1, 10, 20, 30, 40],\n","        'min_samples_split' : [2, 10, 16, 18],\n","        'class_weight': [None, {0:1, 1:2}, {0:1, 1:3}, {0:2, 1:1}, {0:3, 1:1}, {0:2, 1:3}]\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"Decision Tree\"\n","    best_parameters, score = (randomSearch(X,decTreeObj, random_param))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV-LCoTkY9MR","colab_type":"code","colab":{}},"source":["def RandomForestClassifier1(X):\n","    RanForestObj = RandomForestClassifier()\n","    \n","    random_param = {\n","        'n_estimators': [50, 100, 150],\n","        'criterion': ['gini', 'entropy'],\n","        'min_samples_leaf' : [1, 20, 30, 40],\n","        'min_samples_split' : [2, 10, 16],\n","        'class_weight': [None, {0:1, 1:2}, {0:1, 1:3}, {0:2, 1:1}, {0:2, 1:3}], \n","        'bootstrap': [True, False]\n","    }\n","    \n","    fileRow={}\n","    fileRow['name'] = \"Random Forest\"\n","    best_parameters, score = (randomSearch(X,RanForestObj, random_param))\n","    fileRow['Param_Settings'] = best_parameters\n","    fileRow[ScoreCriteria] = score\n","    return fileRow"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1g4aUVRY9MX","colab_type":"text"},"source":["# Making the model"]},{"cell_type":"code","metadata":{"id":"OwSq4AIlY9Ma","colab_type":"code","outputId":"36378363-db8a-43a7-e626-e15fe2b05b09","colab":{}},"source":["#Getting transformed data using LDA, PCA, as well as eliminating selection using backward elimination.\n","X_PCA = myPCA(X) #Can't name them lda or pca otherwise it will conflict with the actual method.\n","X_LDA = myLDA(X)\n","\n","#Add a column of 1 to X before backward elimination\n","X1 = np.append(arr = np.ones([X.shape[0],1]).astype(int), values = X, axis = 1)\n","X_backward=backwardElimination(X1,0.05)\n","#myX_kernel=(X) #Do not run this..."],"execution_count":0,"outputs":[{"output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.137\n","Model:                            OLS   Adj. R-squared:                  0.136\n","Method:                 Least Squares   F-statistic:                     361.2\n","Date:                Tue, 05 May 2020   Prob (F-statistic):               0.00\n","Time:                        00:04:50   Log-Likelihood:                -13459.\n","No. Observations:               29701   AIC:                         2.695e+04\n","Df Residuals:                   29687   BIC:                         2.706e+04\n","Df Model:                          13                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          0.2134      0.002     96.569      0.000       0.209       0.218\n","x1            -0.0162      0.003     -6.049      0.000      -0.022      -0.011\n","x2             0.0064      0.002      2.860      0.004       0.002       0.011\n","x3             0.0503      0.009      5.838      0.000       0.033       0.067\n","x4             0.0466      0.009      5.201      0.000       0.029       0.064\n","x5             0.0318      0.007      4.642      0.000       0.018       0.045\n","x6             0.0152      0.003      6.047      0.000       0.010       0.020\n","x7             0.0084      0.003      3.281      0.001       0.003       0.013\n","x8             0.0920      0.003     29.583      0.000       0.086       0.098\n","x9             0.0366      0.004      9.362      0.000       0.029       0.044\n","x10            0.0207      0.004      5.479      0.000       0.013       0.028\n","x11            0.0171      0.003      5.491      0.000       0.011       0.023\n","x12           -0.0305      0.003    -12.165      0.000      -0.035      -0.026\n","x13           -0.0101      0.002     -4.435      0.000      -0.015      -0.006\n","==============================================================================\n","Omnibus:                     4839.356   Durbin-Watson:                   2.013\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7614.884\n","Skew:                           1.229   Prob(JB):                         0.00\n","Kurtosis:                       3.332   Cond. No.                         11.4\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8K32y19KY9Me","colab_type":"code","outputId":"920b524b-ea53-4a99-efc7-393b9b60f3b4","colab":{}},"source":["resultDict={}\n","\n","for i in ['pca', 'lda', 'back']:\n","    #1: Logistic Regression\n","    #2: kNN\n","    #3: SVM\n","    #4: Naive Bayes\n","    #5: Decision Tree\n","    #6: Random Forest\n","    for j in range(1,7):\n","        if (i == 'pca'):\n","            Regression_And_WriteFiles(X_PCA, 'PCA', j)\n","        elif (i == 'lda'):\n","            Regression_And_WriteFiles(X_LDA, 'LDA', j)\n","        elif (i=='back'):\n","            Regression_And_WriteFiles(X_backward, 'Backward Elimination', j)\n","            "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  % (grid_size, self.n_iter, grid_size), UserWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  % (grid_size, self.n_iter, grid_size), UserWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  % (grid_size, self.n_iter, grid_size), UserWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"AcfHKHWXY9Ml","colab_type":"code","outputId":"ac79e6ff-f0d4-45f6-b612-b59143a481bd","colab":{}},"source":["for i in resultDict:\n","    print(i + \":\" + str(resultDict[i]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["PCA_Random Forest_{'class_weight': None, 'bootstrap': True, 'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 16, 'n_estimators': 150}:0.8239\n","Backward Elimination_Random Forest_{'class_weight': None, 'bootstrap': True, 'criterion': 'entropy', 'min_samples_leaf': 30, 'min_samples_split': 16, 'n_estimators': 150}:0.8279\n","Backward Elimination_SVM_{'degree': 2, 'kernel': 'poly'}:0.8274\n","Backward Elimination_Naive Baynes_{}:0.8038\n","Backward Elimination_kNN_{'n_neighbors': 20, 'metric': 'manhattan'}:0.8222\n","LDA_Random Forest_{'class_weight': {0: 2, 1: 3}, 'bootstrap': False, 'criterion': 'entropy', 'min_samples_leaf': 40, 'min_samples_split': 2, 'n_estimators': 150}:0.8163\n","PCA_Naive Baynes_{}:0.7239\n","LDA_kNN_{'n_neighbors': 20, 'metric': 'minkowski'}:0.8167\n","PCA_Decision Tree_{'min_samples_leaf': 40, 'min_samples_split': 16, 'class_weight': {0: 2, 1: 1}, 'criterion': 'gini'}:0.8102\n","LDA_Logistic Regression_{'class_weight': {0: 2, 1: 3}, 'C': 0.09}:0.8224\n","LDA_Naive Baynes_{}:0.8198\n","LDA_Decision Tree_{'min_samples_leaf': 30, 'min_samples_split': 16, 'class_weight': None, 'criterion': 'entropy'}:0.8146\n","Backward Elimination_Logistic Regression_{'class_weight': {0: 2, 1: 3}, 'C': 25}:0.8231\n","Backward Elimination_Decision Tree_{'min_samples_leaf': 40, 'min_samples_split': 2, 'class_weight': None, 'criterion': 'entropy'}:0.8239\n","PCA_Logistic Regression_{'class_weight': {0: 2, 1: 3}, 'C': 0.001}:0.8226\n","PCA_SVM_{'degree': 4, 'kernel': 'rbf'}:0.8268\n","PCA_kNN_{'n_neighbors': 20, 'metric': 'manhattan'}:0.8184\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"3-YXBe9_Y9Mp","colab_type":"text"},"source":["# Looking at the results, it can be seen that the following 3 models have the highest accuracies"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"igUPnDM3Y9Mq","colab_type":"text"},"source":["1: Backward Elimination - Random Forest(Class_weight:none, bootstrap:true, 'criterion: entropy, min_sample_leaf:30, min_sample_split:16, n_estimators:150)\n","\n","2: Backward Elimination - SVC(kernel=poly, degree=2)\n","\n","3: PCA - SVC(kernel='rbf')\n","\n","For each of the models, as their accuracies are very close, we decided to use kFold with 10 Folds to evaluate each of them based on accuracy, precision, recall and F1 Score"]},{"cell_type":"code","metadata":{"id":"m2elEH-uY9Mt","colab_type":"code","outputId":"90154b45-7396-4f30-edfa-090454461016","colab":{}},"source":["RfModel = RandomForestClassifier(class_weight=None, bootstrap=True, criterion='entropy', min_samples_leaf=30, min_samples_split=16, n_estimators=150)\n","RfScore = kFoldAccuracy(estimator_obj=RfModel, X_data=X_backward, y_data=y)\n","\n","SVMPolyModel = SVC(kernel='poly', degree=2)\n","SVMPolyScore=kFoldAccuracy(estimator_obj=SVMPolyModel, X_data=X_backward, y_data=y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'SVM' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-45-972764fe1c91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mSVMPolyScore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkFoldAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator_obj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVMPolyModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_backward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mSVMRbf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mSVMScore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkFoldAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator_obj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVMRbf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_PCA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'SVM' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GzfZt58vY9Mx","colab_type":"code","outputId":"cd43e303-2332-49aa-9962-7169d716d514","colab":{}},"source":["SVMRbf=SVC()\n","SVMScore=kFoldAccuracy(estimator_obj=SVMRbf, X_data=X_PCA, y_data=y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/home/zfang1216/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JP2LsotoY9M0","colab_type":"code","colab":{}},"source":["Headings = [\"Data Extraction/Selection Method\", \"Model Method\", \"Hyperparameters\", \"Accuracy Score\", \"Recall Score\", \"Precision Score\", \"F1 Score\"]\n","    \n","with open('FinalResult.csv', 'w', newline='') as csvfile:\n","    FinalModelWriter=csv.writer(csvfile)\n","    FinalModelWriter.writerow(Headings)    \n","    writingFiles(\"Backward Elimination\", \"Random Forest\", \"class_weight=None, bootstrap=True, criterion='entropy', min_samples_leaf=30, min_samples_split=16, n_estimators=150\", RfScore)\n","    writingFiles(\"Backward Elimination\", \"SVM\", \"kernel=poly, degree=2\", SVMPolyScore)\n","    writingFiles(\"PCA\", \"RBF SVC\", \"kernel=RBF\",SVMScore)"],"execution_count":0,"outputs":[]}]}